# ğŸ”¬ Experiment Assistant - Assistant RAG pour l'ExpÃ©rimentation

> Assistant bilingue (FR/EN) basÃ© sur RAG pour rÃ©pondre aux questions sur l'expÃ©rimentation en ligne (A/B testing, SRM, FDR, puissance statistique...)

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Dataset](https://img.shields.io/badge/ğŸ¤—-Dataset-yellow)](https://huggingface.co/datasets/lmhdii/experiment-assistant-dataset)

[ğŸ‡¬ğŸ‡§ English version](README_EN.md)

---

## ğŸ¯ ProblÃ©matique

Les Ã©quipes Product et Data Science perdent du temps Ã  chercher des dÃ©finitions fiables sur l'expÃ©rimentation en ligne. Les sources sont dispersÃ©es (Wikipedia, blogs, Slack interne) et les LLM purs peuvent halluciner.

**Solution** : Un assistant RAG (Retrieval-Augmented Generation) qui gÃ©nÃ¨re des rÃ©ponses sourcÃ©es et vÃ©rifiables.

---

## ğŸ” Pourquoi RAG ? VÃ©rifiabilitÃ© avant Confiance Aveugle

> *"Ã€ dÃ©faut d'avoir une IA fiable, on veut une IA vÃ©rifiable"*  
> â€” JIMINI AI (legal tech franÃ§aise)

Ce principe s'applique Ã  l'expÃ©rimentation :
- **DÃ©cisions data-driven** â†’ besoin de sources traÃ§ables
- **Compliance** (RGPD, lÃ©gal) â†’ recommandations auditables  
- **MontÃ©e en compÃ©tence** â†’ les juniors peuvent vÃ©rifier les sources

**RAG = Retrieval + Generation** garantit que chaque rÃ©ponse est vÃ©rifiable via des citations Wikipedia.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gradio    â”‚ â† Interface utilisateur
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangChain        â”‚ â† Orchestration RAG
â”‚  RetrievalQA      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ FAISS  â”‚  â”‚  Groq   â”‚
   â”‚(dense) â”‚  â”‚Llama3.1 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flux** :
1. Question utilisateur â†’ Recherche sÃ©mantique FAISS
2. Top-3 passages Wikipedia â†’ Contexte pour le LLM
3. Llama-3.1 gÃ©nÃ¨re rÃ©ponse + citations cliquables

---

## ğŸ”§ Stack Technique

| Composant | Technologie | Justification |
|-----------|-------------|---------------|
| **LLM** | Llama-3.1-8B-Instant (Groq) | InfÃ©rence rapide, tier gratuit (30k tokens/h) |
| **Framework** | LangChain | Orchestration RAG standard |
| **Vector Store** | FAISS | Recherche sÃ©mantique efficace, pas de serveur |
| **Embeddings** | Sentence-Transformers | Support multilingue (FR/EN) |
| **UI** | Gradio | DÃ©ploiement HuggingFace Spaces simplifiÃ© |
| **HÃ©bergement** | HuggingFace Space | Tier gratuit CPU |

**CoÃ»t total** : 0â‚¬

---

## ğŸ“š Dataset

**17 articles Wikipedia** sÃ©lectionnÃ©s manuellement :
- **10 EN** : A/B testing, False discovery rate, Power (statistics), Multi-armed bandit, Thompson sampling, Sequential analysis, Sample size determination, Equivalence test, Randomized controlled trial, Scientific control
- **7 FR** : Test A/B, Analyse sÃ©quentielle, Puissance statistique, Bandit manchot, Ã‰chantillonnage de Thompson, Essai randomisÃ© contrÃ´lÃ©, Groupe de contrÃ´le

ğŸ“¦ **Dataset public** : [lmhdii/experiment-assistant-dataset](https://huggingface.co/datasets/lmhdii/experiment-assistant-dataset)

---

## ğŸš€ Installation Locale

```bash
# 1. Cloner le projet
git clone https://github.com/lmhdii/experiment-checklist-v2.git
cd experiment-checklist-v2

# 2. CrÃ©er l'environnement virtuel
python3 -m venv .venv
source .venv/bin/activate  # macOS/Linux
# .venv\Scripts\activate  # Windows

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Configurer les clÃ©s API
cp .env.example .env
nano .env  # Ajouter GROQ_API_KEY (gratuit sur console.groq.com)

# 5. Lancer l'application
python app.py
```

Ouvrir http://localhost:7860

---

## ğŸ§ª Exemples de RequÃªtes

| Question | RÃ©ponse attendue | Sources citÃ©es |
|----------|------------------|----------------|
| "Qu'est-ce qu'un SRM ?" | DÃ©finition du Sample Ratio Mismatch | A/B testing (EN), Test A/B (FR) |
| "DiffÃ©rence entre interleaving et A/B ?" | Comparaison des mÃ©thodes | Interleaving (EN), A/B testing (EN) |
| "Comment calculer la puissance statistique ?" | Formule + interprÃ©tation | Power (statistics) (EN), Puissance statistique (FR) |

---

## ğŸ“‚ Structure du Projet

```
experiment-checklist-v2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ build_dataset.py       # Curation Wikipedia (17 articles)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ indexer.py             # CrÃ©ation index FAISS
â”‚   â””â”€â”€ retriever.py           # Recherche de documents
â”œâ”€â”€ faiss_index/               # Index vectoriel (Git LFS)
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”œâ”€â”€ app.py                     # Interface Gradio
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“Š MÃ©triques (MVP v1.0)

| MÃ©trique | Cible | Actuel | Statut |
|----------|-------|--------|--------|
| Latence rÃ©ponse | < 2s | ~1.5s | âœ… |
| Citations systÃ©matiques | 100% | 100% | âœ… |
| Couverture glossaire | 90% | ~45% | âš ï¸ LimitÃ© (17 articles) |
| CoÃ»t infrastructure | 0â‚¬ | 0â‚¬ | âœ… |

---

## âš ï¸ Limitations Actuelles (MVP)

- Corpus limitÃ© Ã  17 articles Wikipedia
- Recherche vectorielle FAISS uniquement (pas de BM25)
- Pas de score de confiance affichÃ©
- Pas de filtre de langue dans l'UI
- Ã‰valuation qualitative uniquement (pas de mÃ©triques automatiques)

---

## ğŸ—“ï¸ Roadmap

| Version | Objectif | Livrable | Date cible |
|---------|----------|----------|------------|
| **v1.0** âœ… | MVP fonctionnel | App Gradio + FAISS + citations | DÃ©c 2024 |
| **v1.1** | Tests unitaires | Pytest + couverture 80%+ | Jan 2025 |
| **v1.2** | Retrieval hybride | FAISS + BM25 + RRF | FÃ©v 2025 |
| **v1.3** | UI avancÃ©e | Score confiance + export JSON | Mar 2025 |
| **v2.0** | Production-ready | CI/CD + monitoring + logs | Avr 2025 |

---

## ğŸ“ Contexte AcadÃ©mique

Projet rÃ©alisÃ© dans le cadre du cours **Data Science** Ã  **DataBird** (2025).

**Adaptation du sujet** : Au lieu d'analyser des tweets sur le changement climatique, j'ai crÃ©Ã© un assistant alignÃ© avec mon rÃ´le de Product Manager en expÃ©rimentation. Le projet respecte tous les critÃ¨res techniques (HuggingFace, LLM, Langchain, Gradio) tout en Ã©tant directement utilisable dans mon travail quotidien.

**Approche de dÃ©veloppement** : En tant que PM, j'ai utilisÃ© Claude (Anthropic) comme co-pilote technique pour l'implÃ©mentation, me concentrant sur l'architecture, les dÃ©cisions produit, et la comprÃ©hension des concepts RAG (retrieval, embeddings, generation). Cette dÃ©marche reflÃ¨te l'utilisation moderne d'outils AI pour le prototypage rapide et le dÃ©veloppement d'une expertise technique adaptÃ©e au rÃ´le de Product Manager.

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Ouvrez une issue ou une pull request.

**Note** : Ce projet a Ã©tÃ© initialement dÃ©veloppÃ© comme prototype PM avec assistance AI. Les contributions de dÃ©veloppeurs pour amÃ©liorer la qualitÃ© du code, optimiser les performances, ou ajouter des features sont particuliÃ¨rement apprÃ©ciÃ©es.

**Axes d'amÃ©lioration prioritaires** :
- Ã‰largir le corpus (blogs techniques, docs internes)
- Ajouter des tests automatisÃ©s
- ImplÃ©menter la recherche hybride

---

## ğŸ“„ Licence

MIT Â© 2025 El Mehdi BELAHNECH

---

## ğŸ”— Liens Utiles

- ğŸ“¦ [Dataset HuggingFace](https://huggingface.co/datasets/lmhdii/experiment-assistant-dataset)
- ğŸš€ [DÃ©mo en ligne](https://huggingface.co/spaces/lmhdii/experiment-assistant) *(Ã  venir)*
- ğŸ’¬ [Issues GitHub](https://github.com/lmhdii/experiment-checklist-v2/issues)

---

**Maintenu avec â¤ï¸ par un PM qui en avait marre de chercher "c'est quoi un SRM" sur Google**